---
title: "Statistical Analysis of Anime Popularity Factors"
author: "Solchanyk Vasyl, Dziuba Oksana, Kuryliak Danylo"
output: html_document
---

```{r setup, include=FALSE}
if (!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
theme_set(theme_minimal())
```

In this mini-research project we analyze factors that influence anime popularity and user ratings. We use the public Anime Recommendations Database from Kaggle, which contains metadata for \~12k anime titles and \~7.8M user–anime interactions. Our goal is to describe the data, understand basic patterns in ratings and popularity, and later test which factors (especially genre) are associated with higher mean ratings and more members.

## Loading the data

We load the anime metadata and user–item rating interactions from the Kaggle Anime Database. We remove unrated entries (rating = -1) and inspect the structure of both datasets to understand variable types and available information.

```{r}
anime <- read_csv("data/anime.csv")
rating <- read_csv(
  "data/rating.csv",
  col_types = cols(
    user_id  = col_integer(),
    anime_id = col_integer(),
    rating   = col_integer()
  )
)

rating <- rating |> filter(rating != -1)

anime |>
  select(anime_id, name, genre, type, episodes, rating, members) |>
  slice(1:10) |>
  knitr::kable(caption = "Example rows from the anime dataset")

rating |>
  slice(1:10) |>
  knitr::kable(caption = "Example rows from the user–anime rating dataset")

vars <- tibble::tribble(
  ~Variable,   ~Dataset, ~Type,      ~Description,
  "anime_id",  "anime",  "integer",  "Unique identifier of an anime title",
  "name",      "anime",  "character","Title of the anime",
  "genre",     "anime",  "character","Comma-separated list of genres",
  "type",      "anime",  "character","TV, Movie, OVA, etc.",
  "episodes",  "anime",  "integer","Number of episodes",
  "rating",    "anime",  "double",   "Mean user rating (1–10)",
  "members",   "anime",  "double",   "Number of users who added the anime",
  "user_id",   "rating", "integer",  "Unique user identifier",
  "rating",    "rating", "integer",  "User’s rating for this anime"
)

knitr::kable(vars, caption = "Variables used in the analysis")
```

## **Distribution of mean ratings**

The histogram of mean ratings shows the overall shape of the rating distribution. This helps identify whether the ratings are symmetric, skewed, tightly clustered, or widely spread.

```{r}
ggplot(anime %>% filter(!is.na(rating)), aes(x = rating)) +
  geom_histogram(bins = 35, fill = "steelblue", alpha = 0.8) +
  labs(
    title = "Distribution of Anime Mean Ratings",
    subtitle = paste("N =", sum(!is.na(anime$rating)), "anime with ratings"),
    x = "Mean rating",
    y = "Count"
  ) +
  theme_minimal()
```

## Distribution of popularity (members)

Popularity is strongly right-skewed, so we use a log scale for clarity. This confirms that a small number of anime dominate in viewership, while most titles have modest audience sizes.

```{r}
ggplot(anime %>% filter(!is.na(members), members > 0), aes(x = members)) +
  geom_histogram(bins = 50, fill = "darkgreen", alpha = 0.7) +
  scale_x_log10() +
  labs(
    title = "Popularity Distribution (log scale)",
    subtitle = paste("N =", sum(!is.na(anime$members) & anime$members > 0), "anime with membership data"),
    x = "Members (log10 scale)",
    y = "Count"
  ) +
  theme_minimal()
```

## Genre comparison - TOP 10

We convert multi-genre strings into individual genre entries and compute the average rating per genre. Displaying only the top ten genres makes the comparison interpretable and highlights which categories tend to receive higher user ratings.

```{r}
genre_summary <- anime %>%
  filter(!is.na(rating), !is.na(genre), genre != "") %>%
  separate_rows(genre, sep = ", ") %>%
  group_by(genre) %>%
  summarise(
    mean_rating = mean(rating, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) %>%
  filter(n >= 30)

top10 <- genre_summary |> 
  arrange(desc(mean_rating)) |> 
  slice_head(n = 10)

ggplot(top10, aes(x = reorder(genre, mean_rating), y = mean_rating)) +
  geom_col(fill = "coral", alpha = 0.8) +
  geom_text(aes(label = sprintf("%.2f", mean_rating)), 
            hjust = 1.1, color = "white", size = 3.5) +
  coord_flip() +
  labs(
    title = "Top 10 Genres by Mean Rating",
    subtitle = "Minimum 30 anime per genre",
    x = "Genre",
    y = "Mean rating"
  ) +
  theme_minimal()
```

## Hypothesis 1 (Does the average rating differ between genres?)

```{r}
# car: Levene's test for equal variances
# FSA: Dunn's test after Kruskal–Wallis
if (!require("car")) install.packages("car")
if (!require("FSA")) install.packages("FSA")
if (!require("knitr")) install.packages("knitr")

library(car)
library(FSA)
library(knitr)

cat("Packages loaded successfully for Hypothesis 1 analysis.\n")
```

## Step 1: Prepare a single-genre variable and keep only rows with a valid rating

```{r}
anime_h1 <- anime %>%
  filter(!is.na(rating), !is.na(genre), genre != "") %>%  # Видаляємо пусті жанри
  mutate(
    main_genre = str_split(genre, ", ") %>%
      map_chr(~ if(length(.x) > 0) .x[1] else NA_character_) %>%
        as.factor()
    ) %>%
  filter(!is.na(main_genre)) 


# Filter to keep only genres with sufficient observations (n >= 10)
genre_counts <- anime_h1 |>
  group_by(main_genre) |>
  summarise(n = n()) |>
  filter(n >= 10)

anime_h1_filtered <- anime_h1 |>
  filter(main_genre %in% genre_counts$main_genre)

cat("Data prepared: ", nrow(anime_h1_filtered), "observations across", 
    length(unique(anime_h1_filtered$main_genre)), "genres\n")
```

## Step 2: Check normality of ratings within each main_genre using KS tests

```{r}
ks_results <- anime_h1_filtered |>
  group_by(main_genre) |>
  summarise(
    n = n(),
    ks_p = ks.test(rating, "pnorm", mean = mean(rating), sd = sd(rating))$p.value,
    .groups = "drop"
  ) |>
  mutate(normality_violated = ks_p < 0.05)

normality_violations <- mean(ks_results$normality_violated)
cat("Normality: ", round(normality_violations * 100, 1), 
    "% of genres violate normality (KS test p < 0.05)\n")
```

Using Kolmogorov–Smirnov tests on mean ratings within each main genre, we found that several large genres (e.g., Action, Adventure, Comedy) have p‑values below 0.05, indicating noticeable deviations from normality and motivating the use of a non‑parametric robustness check alongside ANOVA.

## Step 3: Check homogeneity of variances using Levene's test

```{r}
levene_result <- leveneTest(rating ~ main_genre, data = anime_h1_filtered)
cat("Equal variances: Levene's test p =", 
    format(levene_result$`Pr(>F)`[1], scientific = TRUE), "\n")

if (levene_result$`Pr(>F)`[1] < 0.05) {
  cat("Conclusion: Variances are NOT equal across genres → ANOVA assumption violated\n")
} else {
  cat("Conclusion: Variances appear equal across genres\n")
}
```

**What this means:**\
Both key assumptions for ANOVA are violated. Therefore, **ANOVA results would be unreliable**.

**What tests we should use instead:**

1.  **Kruskal-Wallis test** - Non-parametric version of ANOVA (compares medians/ranks instead of means)

2.  **Dunn's post-hoc test** - For pairwise comparisons after Kruskal-Wallis

3.  **Welch's ANOVA** - An alternative ANOVA that doesn't assume equal variances

```{r}
if (normality_violations > 0.3 || levene_result$`Pr(>F)`[1] < 0.05) {
  cat("\nDecision: Use Kruskal-Wallis test (non-parametric)\n")
  cat("Reason: Violated assumptions (non-normality and/or unequal variances)\n")
} else {
  cat("\nDecision: Use ANOVA (parametric)\n")
  cat("Reason: Assumptions reasonably met\n")
}
```

## **Step 4: Run the Kruskal-Wallis Test**

```{r}
kruskal_result <- kruskal.test(rating ~ main_genre, data = anime_h1_filtered)

cat("\nKruskal-Wallis Test Results:\n")
cat("H-statistic =", round(kruskal_result$statistic, 2), "\n")
cat("p-value =", format(kruskal_result$p.value, scientific = TRUE), "\n")

# Effect size
H <- kruskal_result$statistic
k <- length(unique(anime_h1_filtered$main_genre))
N <- nrow(anime_h1_filtered)
epsilon_sq <- (H - k + 1) / (N - k)
cat("Effect size (ε²) =", round(epsilon_sq, 4), "\n")

# Hypothesis decision
alpha <- 0.05
if (kruskal_result$p.value < alpha) {
  cat("\nConclusion: REJECT H₀ - Ratings differ significantly across genres\n")
} else {
  cat("\nConclusion: FAIL TO REJECT H₀ - No significant difference in ratings across genres\n")
}
```

## Step 5: Post-hoc analysis

```{r}
if (kruskal_result$p.value < alpha) {
  dunn_result <- dunnTest(rating ~ main_genre, 
                         data = anime_h1_filtered,
                         method = "bonferroni")
  
  significant_comparisons <- dunn_result$res |>
    filter(P.adj < 0.05)
  
  cat("\nPost-hoc (Dunn's test):", nrow(significant_comparisons), 
      "significant pairwise comparisons found\n")
  
  if (nrow(significant_comparisons) > 0) {
    # Show top 5 most significant comparisons
    cat("\nTop 5 most significant genre differences:\n")
    significant_comparisons |>
      arrange(P.adj) |>
      mutate(`Adj. p` = format(P.adj, scientific = TRUE, digits = 2)) |>
      select(Comparison, Z, `Adj. p`) |>
      slice_head(n = 5) |>
      knitr::kable(row.names = FALSE, digits = 3)
  }
}
```

## **Hypothesis 1: Results Summary**

### **Test Conducted:**

-   **Null Hypothesis (H₀):** Mean ratings are equal across all genres

-   **Alternative Hypothesis (H₁):** At least one genre pair shows significant difference in mean ratings

### **Assumption Checks:**

-   Normality: 12.5% of genres violate normality (KS test p \< 0.05)

-   Equal variances: **NOT equal** across genres (Levene's test p = 3.9e-41)

-   **Decision:** Use Kruskal-Wallis non-parametric test

### **Test Results:**

-   Kruskal-Wallis H-statistic = 1525.5

-   **p-value = 1.46e-302** (highly significant)

-   Effect size ε² = 0.1244 (medium effect)

### **Conclusion:**

**REJECT H0** - Ratings differ significantly across genres

### **Post-hoc Analysis (Dunn's test):**

-   Many significant pairwise comparisons found

-   **Top example:** Action vs Kids (Z = 21.81, p = 8.7e-103)

-   Other significant differences: Action-Dementia, Dementia-Drama, Adventure-Dementia, Drama-Kids

### **Interpretation:**

1.  **Statistical significance:** Strong evidence that genre affects ratings

2.  **Practical significance:** Medium effect size suggests meaningful differences

3.  **Key finding:** Certain genres (like Action) tend to have systematically different ratings than others (like Kids)
